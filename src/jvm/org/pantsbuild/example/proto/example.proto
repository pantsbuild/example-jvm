syntax = "proto3";

option java_multiple_files = true;
option java_package = "org.pantsbuild.example";
option java_outer_classname = "DatabricksAPIProto";

package org.pantsbuild.example;


message SubmitPynestJobRequest {
  // To deduplicate retried job submits. This will be a UUID token generated by the requester
  string idempotency_token = 1;

  // The git SHA commit id of the job you want to run
  string git_sha = 2;

  // The preregisterd job name to run
  string job_name = 3;

  // The service role to run the job with: e.g. etl, secure_etl, etc.
  // If the caller does not have access to the role the request will be rejected
  string requested_role = 4;

  // A list of arguments to pass to the job
  repeated string task_args = 5;

  // Config service scope
  string config_scope = 6;

  // The jar version for a data/jms-package-manager project that you want to run
  string jmspm_jar_version = 100;
}

message SubmitRawJobRequest {
  // To deduplicate retried job submits. This will be a UUID token generated by the requester
  string idempotency_token = 1;

  // The service role to run the job with: e.g. etl, secure_etl, etc.
  // If the caller does not have access to the role the request will be rejected
  string requested_role = 2;

  // Dependent libraries (jar, wheel) for the job
  repeated JobLibrary libraries = 3;

  // Spark driver application file (jar, wheel)
  string application_file = 4;

  // main class name for the driver application
  string main_class_name = 5;

  // Arguments to the Spark application
  repeated string application_args = 6;

  // Config service scope name
  string config_scope = 7;

  // Spark config
  map<string, string> spark_conf = 10;

  // Spark environments
  map<string, string> spark_env = 11;

  // Python files
  repeated string py_files = 12;

  // files
  repeated string files = 13;

  // zip files that contains the Python dependencies
  repeated string archives = 14;

  // Existing cluster id
  string existing_cluster_id = 20;

  // New job cluster request
  JobClusterRequest new_cluster = 21;

  // maximum retries
  int32 max_retries = 30;

  // Email to notify upon job failures
  EmailNotifications email_notifications = 31;

  // Extra developer data for Job Proxy to interact with Databricks server. It is only used by Job Proxy developers.
  // For example, when we want to send some extra data together with job request which Job Proxy sends to
  // Databricks, we could put into this field. Normal Spark user should never use this field.
  string developer_metadata = 100;
}

message JobLibrary {
  string jar = 1;
  string whl = 2;
}

message JobClusterRequest {
  int32 min_workers = 1;
  int32 max_workers = 2;
  string node_type_id = 3;
  string driver_node_type_id = 4;
  string spark_version = 5;
  string instance_pool_id = 6;
  string driver_instance_pool_id = 7;
  AwsAttributes aws_attributes = 100;
}

message EmailNotifications {
  repeated string on_start = 1;
  repeated string on_success = 2;
  repeated string on_failure = 3;
}

message AwsAttributes {
  int32 first_on_demand = 1;
  string availability = 2;
  string zone_id = 3;
}

// SubmitJobResponse contains info to retrive a submitted job
message SubmitJobResponse {
  // This name is generated by the proxy.
  string run_name = 1;
  // This name is generated by databricks, and is the the lookup key for the run
  string run_id = 2;
  // This name is generated by databricks, and is the the lookup key for the job
  string job_id = 3;
}

message GetJobStateRequest {
  // The run id returned by SubmitJob
  string run_id = 1;
}

message GetJobStateResponse {
  // state of the cluster
  string life_cycle_state = 1;
  // result of the job
  string result_state = 2;
  // state message or error message
  string state_message = 3;
  // user cancelled or timed out
  bool user_cancelled_or_timedout = 4;
}

message CancelRunRequest {
  string run_id = 1;
}

message CancelRunResponse {
}

message QueryJobRequest {
  string service_name = 1;
  string unique_app_name = 2;
  // if job_name is provided, will query job by name and ignore service_name and unique_app_name
  string job_name = 3;
  int32  max_result = 100;
}

message QueryJobResponse {
  repeated JobSummary jobs = 1;
}

message JobSummary {
  string job_id = 1;
  repeated JobRunSummary active_runs = 2;
}

message JobRunSummary {
  string run_id = 1;
  string state = 2;
  string message = 3;
}

message DeleteJobRequest {
  string job_id = 1;
}

message DeleteJobResponse {
}

message ListAllActiveJobRunsRequest {
  string run_name = 1;
}

message JobStateSummary {
  string life_cycle_state = 1;
  string state_message = 2;
  bool user_cancelled_or_timedout = 3;
}

message ClusterSpecSummary {
  NewClusterSummary new_cluster = 1;
  repeated JobLibrary libraries = 2;
}

message NewClusterSummary {
  string driver_node_type_id = 1;
  string node_type_id = 2;
}

message ClusterInstanceSummary {
  string cluster_id = 1;
}

message TaskSummary {
  SparkJarTaskSummary spark_jar_task = 1;
  SparkSubmitTaskSummary spark_submit_task = 2;
}

message SparkJarTaskSummary {
  repeated string parameters = 1;
}

message SparkSubmitTaskSummary {
  repeated string parameters = 1;
}

message AllJobsRunSummary {
  string job_id = 1;
  string run_id = 2;
  string run_name = 3;
  string start_time = 4;
  string creator_user_name = 5;
  TaskSummary task = 6;
  ClusterSpecSummary cluster_spec = 7;
  ClusterInstanceSummary cluster_instance = 8;
  JobStateSummary state = 9;
}

message ListAllActiveJobRunsResponse {
  repeated AllJobsRunSummary active_runs = 1;
}

// The DatabricksJobProxyService service definition
service DatabricksJobProxyService {
  // Submit a job from pynest
  rpc SubmitPynestJob (SubmitPynestJobRequest) returns (SubmitJobResponse);
  // Submit a raw job with application file url and related Spark configuration/arguments
  rpc SubmitRawJob (SubmitRawJobRequest) returns (SubmitJobResponse);
  // Get job status
  rpc GetJobState (GetJobStateRequest) returns (GetJobStateResponse);
  // Cancel a job run
  rpc CancelRun (CancelRunRequest) returns (CancelRunResponse);
  // Query a job
  rpc QueryJob (QueryJobRequest) returns (QueryJobResponse);
  // Delete a job
  rpc DeleteJob (DeleteJobRequest) returns (DeleteJobResponse);
  // List all active job runs
  rpc ListAllActiveJobRuns (ListAllActiveJobRunsRequest) returns (ListAllActiveJobRunsResponse);
}
